{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a872b11",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2901eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (1345727, 768)\n",
      "Labels shape: (1345727,)\n",
      "\n",
      "Example label: neutral\n",
      "Example vector (first 5 values): [-0.35439885 -0.23273325  0.01678974 -0.5580163   0.07386743]\n"
     ]
    }
   ],
   "source": [
    "# Loading the final dataset with PCA reduced and embeddings\n",
    "df = pd.read_pickle(\"/Users/diegolemos/Masters/Theses/code/data/processed/final_model_dataset.pkl\")\n",
    "\n",
    "# Converting 'embedding' column to 2D NumPy array\n",
    "X = np.vstack(df['embedding'].values)\n",
    "\n",
    "# Extracting the target labels\n",
    "y = df['final_sentiment'].values\n",
    "\n",
    "# Previewing shapes\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "# Checking rows\n",
    "print(\"\\nExample label:\", y[0])\n",
    "print(\"Example vector (first 5 values):\", X[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc469d",
   "metadata": {},
   "source": [
    "### Train and Tests Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df22729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (1076581, 768)\n",
      "Test set size: (269146, 768)\n"
     ]
    }
   ],
   "source": [
    "# Train test split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913ad46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (942008, 768)\n",
      "Test set size: (403719, 768)\n"
     ]
    }
   ],
   "source": [
    "# Train test split 70/30\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train set size:\", X_train_70.shape)\n",
    "print(\"Test set size:\", X_test_30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc597491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1009295, 768)\n",
      "Test size: (336432, 768)\n"
     ]
    }
   ],
   "source": [
    "# Train test split 75/25\n",
    "X_train_75, X_test_25, y_train_75, y_test_25 = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train size:\", X_train_75.shape)\n",
    "print(\"Test size:\", X_test_25.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d62c19",
   "metadata": {},
   "source": [
    "### Support Verctor Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3b5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.76      0.65    153371\n",
      "     neutral       0.25      0.14      0.18     65563\n",
      "    positive       0.20      0.12      0.15     50212\n",
      "\n",
      "    accuracy                           0.49    269146\n",
      "   macro avg       0.34      0.34      0.33    269146\n",
      "weighted avg       0.42      0.49      0.44    269146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Standardizing features for 80/20 split\n",
    "scaler_80 = StandardScaler()\n",
    "X_train_scaled = scaler_80.fit_transform(X_train)\n",
    "X_test_scaled = scaler_80.transform(X_test)\n",
    "\n",
    "# Train SVM using SGD Classifier\n",
    "svm_80 = SGDClassifier( loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "svm_80.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred_svm_80 = svm_80.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating\n",
    "report_svm = classification_report(y_test, y_pred_svm_80, output_dict=True)\n",
    "print(classification_report(y_test, y_pred_svm_80))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.62      0.59    230057\n",
      "     neutral       0.25      0.12      0.17     98344\n",
      "    positive       0.18      0.26      0.21     75318\n",
      "\n",
      "    accuracy                           0.43    403719\n",
      "   macro avg       0.33      0.33      0.32    403719\n",
      "weighted avg       0.42      0.43      0.42    403719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardizing features for 70/30 split\n",
    "scaler_70 = StandardScaler()\n",
    "X_train_70_scaled = scaler_70.fit_transform(X_train_70)\n",
    "X_test_30_scaled = scaler_70.transform(X_test_30)\n",
    "\n",
    "# Train SVM using SGD Classifier\n",
    "svm_70 = SGDClassifier( loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "svm_70.fit(X_train_70_scaled, y_train_70)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred_svm_70 = svm_70.predict(X_test_30_scaled)\n",
    "\n",
    "# Evaluating\n",
    "report_svm_70 = classification_report(y_test_30, y_pred_svm_70, output_dict=True)\n",
    "print(classification_report(y_test_30, y_pred_svm_70))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a4448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.82      0.67    191714\n",
      "     neutral       0.24      0.07      0.10     81953\n",
      "    positive       0.18      0.11      0.14     62765\n",
      "\n",
      "    accuracy                           0.51    336432\n",
      "   macro avg       0.33      0.33      0.30    336432\n",
      "weighted avg       0.42      0.51      0.43    336432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardize features for 75/25 split\n",
    "scaler_75 = StandardScaler()\n",
    "X_train_75_scaled = scaler_75.fit_transform(X_train_75)\n",
    "X_test_25_scaled = scaler_75.transform(X_test_25)\n",
    "\n",
    "# Train SVM using SGD Classifier\n",
    "svm_75 = SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "svm_75.fit(X_train_75_scaled, y_train_75)\n",
    "\n",
    "# Predict\n",
    "y_pred_svm_75 = svm_75.predict(X_test_25_scaled)\n",
    "\n",
    "# Evaluating\n",
    "report_svm_75 = classification_report(y_test_25, y_pred_svm_75, output_dict=True)\n",
    "print(classification_report(y_test_25, y_pred_svm_75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72554da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM results saved.\n"
     ]
    }
   ],
   "source": [
    "# Saving classification report\n",
    "with open(\"/Users/diegolemos/Masters/Theses/code/results/sgd_svm_results_80_20.json\", \"w\") as f:\n",
    "    json.dump(report_svm, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test, y_pred_svm, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"SGD SVM Confusion Matrix (80/20 Split)\")\n",
    "plt.savefig(\"/Users/diegolemos/Masters/Theses/code/results/sgd_svm_confusion_matrix_80_20.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"/Users/diegolemos/Masters/Theses/code/results/sgd_svm_confusion_matrix_80_20.csv\")\n",
    "\n",
    "print(\"SVM results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd7531a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/30 SVM results saved.\n"
     ]
    }
   ],
   "source": [
    "# Save classification report\n",
    "with open(\"../results/sgd_svm_results_70_30.json\", \"w\") as f:\n",
    "    json.dump(report_svm_70, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test_30, y_pred_svm_70, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"SGD SVM Confusion Matrix (70/30 Split)\")\n",
    "plt.savefig(\"../results/sgd_svm_confusion_matrix_70_30.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/sgd_svm_confusion_matrix_70_30.csv\")\n",
    "\n",
    "print(\"70/30 SVM results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a0da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/25 SVM results saved.\n"
     ]
    }
   ],
   "source": [
    "# Save classification report\n",
    "with open(\"../results/sgd_svm_results_75_25.json\", \"w\") as f:\n",
    "    json.dump(report_svm_75, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test_25, y_pred_svm_75, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"SGD SVM Confusion Matrix (75/25 Split)\")\n",
    "plt.savefig(\"../results/sgd_svm_confusion_matrix_75_25.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/sgd_svm_confusion_matrix_75_25.csv\")\n",
    "\n",
    "print(\"75/25 SVM results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f534f",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb857b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.54      0.55    153371\n",
      "     neutral       0.24      0.33      0.28     65563\n",
      "    positive       0.20      0.13      0.16     50212\n",
      "\n",
      "    accuracy                           0.41    269146\n",
      "   macro avg       0.34      0.33      0.33    269146\n",
      "weighted avg       0.42      0.41      0.41    269146\n",
      "\n",
      "\n",
      " Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.76      0.65    153371\n",
      "     neutral       0.24      0.09      0.13     65562\n",
      "    positive       0.18      0.15      0.16     50213\n",
      "\n",
      "    accuracy                           0.48    269146\n",
      "   macro avg       0.33      0.33      0.31    269146\n",
      "weighted avg       0.42      0.48      0.43    269146\n",
      "\n",
      "\n",
      " Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.77      0.66    153371\n",
      "     neutral       0.24      0.22      0.23     65562\n",
      "    positive       0.19      0.01      0.02     50212\n",
      "\n",
      "    accuracy                           0.50    269145\n",
      "   macro avg       0.33      0.33      0.30    269145\n",
      "weighted avg       0.42      0.50      0.43    269145\n",
      "\n",
      "\n",
      " Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.76      0.65    153371\n",
      "     neutral       0.25      0.07      0.11     65562\n",
      "    positive       0.19      0.17      0.18     50212\n",
      "\n",
      "    accuracy                           0.48    269145\n",
      "   macro avg       0.34      0.33      0.31    269145\n",
      "weighted avg       0.42      0.48      0.43    269145\n",
      "\n",
      "\n",
      " Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.69      0.63    153371\n",
      "     neutral       0.25      0.15      0.19     65562\n",
      "    positive       0.20      0.18      0.19     50212\n",
      "\n",
      "    accuracy                           0.46    269145\n",
      "   macro avg       0.34      0.34      0.33    269145\n",
      "weighted avg       0.42      0.46      0.44    269145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting up 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#Storagie\n",
    "svm_fold_reports = []\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(f\"\\n Fold {fold}\")\n",
    "    \n",
    "    X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
    "    y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
    "    \n",
    "    #Standardizing features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_cv_scaled = scaler.fit_transform(X_train_cv)\n",
    "    X_test_cv_scaled = scaler.transform(X_test_cv)\n",
    "    \n",
    "    #SVM\n",
    "    svm_cv = SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3, random_state=42)\n",
    "    \n",
    "    #Training\n",
    "    svm_cv.fit(X_train_cv_scaled, y_train_cv)\n",
    "    y_pred_cv = svm_cv.predict(X_test_cv_scaled)\n",
    "    \n",
    "    #Classification report\n",
    "    report = classification_report(y_test_cv, y_pred_cv, output_dict=True)\n",
    "    svm_fold_reports.append(report)\n",
    "    \n",
    "    print(classification_report(y_test_cv, y_pred_cv))\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79251f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold SVM average results saved.\n",
      "\n",
      "{\n",
      "    \"negative\": {\n",
      "        \"precision\": 0.5705,\n",
      "        \"recall\": 0.7027,\n",
      "        \"f1-score\": 0.6273\n",
      "    },\n",
      "    \"neutral\": {\n",
      "        \"precision\": 0.2451,\n",
      "        \"recall\": 0.1719,\n",
      "        \"f1-score\": 0.1873\n",
      "    },\n",
      "    \"positive\": {\n",
      "        \"precision\": 0.1883,\n",
      "        \"recall\": 0.1277,\n",
      "        \"f1-score\": 0.1404\n",
      "    },\n",
      "    \"macro avg F1\": 0.3183,\n",
      "    \"weighted avg F1\": 0.4293\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Extracting F1, precision, recall for each class across folds\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
    "\n",
    "#Initializing dictionary\n",
    "avg_results = {label: {metric: [] for metric in metrics} for label in labels}\n",
    "\n",
    "#Accumulating metrics across folds\n",
    "for report in svm_fold_reports:\n",
    "    for label in labels:\n",
    "        for metric in metrics:\n",
    "            avg_results[label][metric].append(report[label][metric])\n",
    "\n",
    "#Computing averages\n",
    "avg_results_final = {\n",
    "    label: {\n",
    "        metric: round(np.mean(avg_results[label][metric]), 4)\n",
    "        for metric in metrics\n",
    "    }\n",
    "    for label in labels\n",
    "}\n",
    "\n",
    "#Adding overall averages\n",
    "macro_f1 = np.mean([avg_results_final[label]['f1-score'] for label in labels])\n",
    "weighted_f1s = [report[\"weighted avg\"][\"f1-score\"] for report in svm_fold_reports]\n",
    "\n",
    "avg_results_final[\"macro avg F1\"] = round(macro_f1, 4)\n",
    "avg_results_final[\"weighted avg F1\"] = round(np.mean(weighted_f1s), 4)\n",
    "\n",
    "#Saving as JSON\n",
    "with open(\"../results/sgd_svm_results_5fold.json\", \"w\") as f:\n",
    "    json.dump(avg_results_final, f, indent=4)\n",
    "\n",
    "# Display summary\n",
    "print(\"5-Fold SVM average results saved.\\n\")\n",
    "print(json.dumps(avg_results_final, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1feebe0",
   "metadata": {},
   "source": [
    "#### SVM Performance\n",
    "On PCA-reduced BERT embeddings, the SVM model was fine-tuned with 5 fold cross validation. It was seen that although the model could indeed pick up some signal, it performed much worse than it should have. This implies that the dimensionality reduction might have eliminated feature required to correctly label them. Still, the SVM offered a practical point of reference among the conventional classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a164fb",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c89f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.91      0.70    153371\n",
      "     neutral       0.25      0.05      0.08     65563\n",
      "    positive       0.19      0.04      0.07     50212\n",
      "\n",
      "    accuracy                           0.54    269146\n",
      "   macro avg       0.34      0.33      0.28    269146\n",
      "weighted avg       0.42      0.54      0.43    269146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Training Logistic Regression using SGD 80/20\n",
    "logreg_model = SGDClassifier(loss=\"log_loss\", class_weight=\"balanced\", max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Training\n",
    "logreg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predicting\n",
    "y_pred_logreg = logreg_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating\n",
    "report_logreg = classification_report(y_test, y_pred_logreg, output_dict=True)\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2943f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_70_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m logreg_70 = SGDClassifier(loss=\u001b[33m\"\u001b[39m\u001b[33mlog_loss\u001b[39m\u001b[33m\"\u001b[39m, class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, max_iter=\u001b[32m1000\u001b[39m, tol=\u001b[32m1e-3\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m logreg_70.fit(\u001b[43mX_train_70_scaled\u001b[49m, y_train_70)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Predicting\u001b[39;00m\n\u001b[32m      8\u001b[39m y_pred_logreg_70 = logreg_70.predict(X_test_30_scaled)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_70_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression using SGD 70/30\n",
    "logreg_70 = SGDClassifier(loss=\"log_loss\", class_weight=\"balanced\", max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Training\n",
    "logreg_70.fit(X_train_70_scaled, y_train_70)\n",
    "\n",
    "# Predicting\n",
    "y_pred_logreg_70 = logreg_70.predict(X_test_30_scaled)\n",
    "\n",
    "# Evaluating\n",
    "report_logreg_70 = classification_report(y_test_30, y_pred_logreg_70, output_dict=True)\n",
    "print(classification_report(y_test_30, y_pred_logreg_70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa49b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.83      0.67    191714\n",
      "     neutral       0.24      0.06      0.10     81953\n",
      "    positive       0.18      0.11      0.14     62765\n",
      "\n",
      "    accuracy                           0.51    336432\n",
      "   macro avg       0.33      0.33      0.30    336432\n",
      "weighted avg       0.42      0.51      0.43    336432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression using SGD\n",
    "logreg_75 = SGDClassifier(loss=\"log_loss\", class_weight=\"balanced\", max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Training\n",
    "logreg_75.fit(X_train_75_scaled, y_train_75)\n",
    "\n",
    "# Predict\n",
    "y_pred_logreg_75 = logreg_75.predict(X_test_25_scaled)\n",
    "\n",
    "# Evaluating\n",
    "report_logreg_75 = classification_report(y_test_25, y_pred_logreg_75, output_dict=True)\n",
    "print(classification_report(y_test_25, y_pred_logreg_75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "700d2367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression results saved.\n"
     ]
    }
   ],
   "source": [
    "# Saving classification report\n",
    "with open(\"../results/logreg_results_80_20.json\", \"w\") as f:\n",
    "    json.dump(report_logreg, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test, y_pred_logreg, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression Confusion Matrix (80/20 Split)\")\n",
    "plt.savefig(\"../results/logreg_confusion_matrix_80_20.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/logreg_confusion_matrix_80_20.csv\")\n",
    "\n",
    "print(\"Logistic Regression results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3319285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/30 Logistic Regression results saved.\n"
     ]
    }
   ],
   "source": [
    "# Saving classification report\n",
    "with open(\"../results/logreg_results_70_30.json\", \"w\") as f:\n",
    "    json.dump(report_logreg_70, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test_30, y_pred_logreg_70, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression Confusion Matrix (70/30 Split)\")\n",
    "plt.savefig(\"../results/logreg_confusion_matrix_70_30.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/logreg_confusion_matrix_70_30.csv\")\n",
    "\n",
    "print(\"70/30 Logistic Regression results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49438cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/25 Logistic Regression results saved.\n"
     ]
    }
   ],
   "source": [
    "# Save classification report\n",
    "with open(\"../results/logreg_results_75_25.json\", \"w\") as f:\n",
    "    json.dump(report_logreg_75, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test_25, y_pred_logreg_75, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression Confusion Matrix (75/25 Split)\")\n",
    "plt.savefig(\"../results/logreg_confusion_matrix_75_25.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/logreg_confusion_matrix_75_25.csv\")\n",
    "\n",
    "print(\"75/25 Logistic Regression results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62282b",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42a7968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.88      0.69    153371\n",
      "     neutral       0.25      0.08      0.12     65563\n",
      "    positive       0.19      0.05      0.08     50212\n",
      "\n",
      "    accuracy                           0.53    269146\n",
      "   macro avg       0.34      0.33      0.30    269146\n",
      "weighted avg       0.42      0.53      0.44    269146\n",
      "\n",
      "\n",
      " Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.86      0.69    153371\n",
      "     neutral       0.24      0.10      0.14     65562\n",
      "    positive       0.19      0.04      0.07     50213\n",
      "\n",
      "    accuracy                           0.52    269146\n",
      "   macro avg       0.33      0.33      0.30    269146\n",
      "weighted avg       0.42      0.52      0.44    269146\n",
      "\n",
      "\n",
      " Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.87      0.69    153371\n",
      "     neutral       0.24      0.03      0.05     65562\n",
      "    positive       0.19      0.10      0.13     50212\n",
      "\n",
      "    accuracy                           0.52    269145\n",
      "   macro avg       0.33      0.33      0.29    269145\n",
      "weighted avg       0.42      0.52      0.43    269145\n",
      "\n",
      "\n",
      " Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.87      0.69    153371\n",
      "     neutral       0.24      0.02      0.03     65562\n",
      "    positive       0.20      0.13      0.15     50212\n",
      "\n",
      "    accuracy                           0.52    269145\n",
      "   macro avg       0.34      0.34      0.29    269145\n",
      "weighted avg       0.42      0.52      0.43    269145\n",
      "\n",
      "\n",
      " Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.79      0.66    153371\n",
      "     neutral       0.25      0.15      0.19     65562\n",
      "    positive       0.19      0.07      0.10     50212\n",
      "\n",
      "    accuracy                           0.50    269145\n",
      "   macro avg       0.34      0.34      0.32    269145\n",
      "weighted avg       0.42      0.50      0.44    269145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting up 5-fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "logreg_fold_reports = []\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(f\"\\n Fold {fold}\")\n",
    "    \n",
    "    X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
    "    y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
    "    \n",
    "    #Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_cv_scaled = scaler.fit_transform(X_train_cv)\n",
    "    X_test_cv_scaled = scaler.transform(X_test_cv)\n",
    "    \n",
    "    #Logistic Regression model\n",
    "    logreg_cv = SGDClassifier(loss=\"log_loss\", class_weight=\"balanced\", max_iter=1000, tol=1e-3, random_state=42)\n",
    "    \n",
    "    #Training\n",
    "    logreg_cv.fit(X_train_cv_scaled, y_train_cv)\n",
    "    y_pred_cv = logreg_cv.predict(X_test_cv_scaled)\n",
    "    \n",
    "    #Storing report\n",
    "    report = classification_report(y_test_cv, y_pred_cv, output_dict=True)\n",
    "    logreg_fold_reports.append(report)\n",
    "    \n",
    "    print(classification_report(y_test_cv, y_pred_cv))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb787760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Logistic Regression results saved.\n",
      "\n",
      "{\n",
      "    \"negative\": {\n",
      "        \"precision\": 0.5708,\n",
      "        \"recall\": 0.8523,\n",
      "        \"f1-score\": 0.6834\n",
      "    },\n",
      "    \"neutral\": {\n",
      "        \"precision\": 0.2444,\n",
      "        \"recall\": 0.074,\n",
      "        \"f1-score\": 0.1048\n",
      "    },\n",
      "    \"positive\": {\n",
      "        \"precision\": 0.1923,\n",
      "        \"recall\": 0.0779,\n",
      "        \"f1-score\": 0.1072\n",
      "    },\n",
      "    \"macro avg F1\": 0.2985,\n",
      "    \"weighted avg F1\": 0.435\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Labels and metrics\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
    "\n",
    "#Initializing dictionary\n",
    "avg_logreg_results = {label: {metric: [] for metric in metrics} for label in labels}\n",
    "\n",
    "#Collecting metrics from each fold\n",
    "for report in logreg_fold_reports:\n",
    "    for label in labels:\n",
    "        for metric in metrics:\n",
    "            avg_logreg_results[label][metric].append(report[label][metric])\n",
    "\n",
    "#Computing average per label/metric\n",
    "avg_logreg_final = {\n",
    "    label: {\n",
    "        metric: round(np.mean(avg_logreg_results[label][metric]), 4)\n",
    "        for metric in metrics\n",
    "    }\n",
    "    for label in labels\n",
    "}\n",
    "\n",
    "#Adding macro and weighted avg\n",
    "macro_f1 = np.mean([avg_logreg_final[label]['f1-score'] for label in labels])\n",
    "weighted_f1s = [report[\"weighted avg\"][\"f1-score\"] for report in logreg_fold_reports]\n",
    "\n",
    "avg_logreg_final[\"macro avg F1\"] = round(macro_f1, 4)\n",
    "avg_logreg_final[\"weighted avg F1\"] = round(np.mean(weighted_f1s), 4)\n",
    "\n",
    "# Saving to JSON\n",
    "with open(\"../results/logreg_results_5fold.json\", \"w\") as f:\n",
    "    json.dump(avg_logreg_final, f, indent=4)\n",
    "\n",
    "# Final summary\n",
    "print(\"5-Fold Logistic Regression results saved.\\n\")\n",
    "print(json.dumps(avg_logreg_final, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62ca0c",
   "metadata": {},
   "source": [
    "#### LR Performance\n",
    "Logistic Regression also did not do so well with the PCA-reduced embeddings. The model was simple and the decision boundary of the model is linear, hence, it might have hindered its performance to represent non-linear patterns in the feature space. The findings support the suspicion that flattening out and dimension reduction of BERT embedding can eliminate precious semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3751bb",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    153371\n",
      "     neutral       0.33      0.00      0.00     65563\n",
      "    positive       0.21      0.00      0.00     50212\n",
      "\n",
      "    accuracy                           0.57    269146\n",
      "   macro avg       0.37      0.33      0.24    269146\n",
      "weighted avg       0.45      0.57      0.41    269146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest 80/20\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "\n",
    "#Training\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#Predicting\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "#Evaluating\n",
    "report_rf = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    230057\n",
      "     neutral       0.37      0.00      0.00     98344\n",
      "    positive       0.24      0.00      0.00     75318\n",
      "\n",
      "    accuracy                           0.57    403719\n",
      "   macro avg       0.40      0.33      0.24    403719\n",
      "weighted avg       0.46      0.57      0.41    403719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest 70/30\n",
    "rf_70 = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42,n_jobs=-1)\n",
    "\n",
    "#Training\n",
    "rf_70.fit(X_train_70, y_train_70)\n",
    "\n",
    "#Predicting\n",
    "y_pred_rf_70 = rf_70.predict(X_test_30)\n",
    "\n",
    "#Evaluating\n",
    "report_rf_70 = classification_report(y_test_30, y_pred_rf_70, output_dict=True)\n",
    "print(classification_report(y_test_30, y_pred_rf_70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b1f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    191714\n",
      "     neutral       0.31      0.00      0.00     81953\n",
      "    positive       0.19      0.00      0.00     62765\n",
      "\n",
      "    accuracy                           0.57    336432\n",
      "   macro avg       0.36      0.33      0.24    336432\n",
      "weighted avg       0.44      0.57      0.41    336432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest 75/25 split \n",
    "rf_75 = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "\n",
    "#Training\n",
    "rf_75.fit(X_train_75, y_train_75)\n",
    "\n",
    "#Predicting\n",
    "y_pred_rf_75 = rf_75.predict(X_test_25)\n",
    "\n",
    "#Evaluating\n",
    "report_rf_75 = classification_report(y_test_25, y_pred_rf_75, output_dict=True)\n",
    "print(classification_report(y_test_25, y_pred_rf_75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810ece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest results saved.\n"
     ]
    }
   ],
   "source": [
    "#Saving classification report\n",
    "with open(\"../results/rf_results_80_20.json\", \"w\") as f:\n",
    "    json.dump(report_rf, f, indent=4)\n",
    "\n",
    "#Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test, y_pred_rf, labels=labels)\n",
    "\n",
    "#Saving confusion matrix image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Random Forest Confusion Matrix (80/20 Split)\")\n",
    "plt.savefig(\"../results/rf_confusion_matrix_80_20.png\")\n",
    "plt.close()\n",
    "\n",
    "#Saving as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/rf_confusion_matrix_80_20.csv\")\n",
    "\n",
    "print(\"Random Forest results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/30 Random Forest results saved.\n"
     ]
    }
   ],
   "source": [
    "#Saving classification report\n",
    "with open(\"../results/rf_results_70_30.json\", \"w\") as f:\n",
    "    json.dump(report_rf_70, f, indent=4)\n",
    "\n",
    "#Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test_30, y_pred_rf_70, labels=labels)\n",
    "\n",
    "#Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Random Forest Confusion Matrix (70/30 Split)\")\n",
    "plt.savefig(\"../results/rf_confusion_matrix_70_30.png\")\n",
    "plt.close()\n",
    "\n",
    "#Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/rf_confusion_matrix_70_30.csv\")\n",
    "\n",
    "print(\"70/30 Random Forest results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f1ebf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/25 Random Forest results saved.\n"
     ]
    }
   ],
   "source": [
    "#Saving classification report\n",
    "with open(\"../results/rf_results_75_25.json\", \"w\") as f:\n",
    "    json.dump(report_rf_75, f, indent=4)\n",
    "\n",
    "# Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test_25, y_pred_rf_75, labels=labels)\n",
    "\n",
    "# Saving confusion matrix as image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Random Forest Confusion Matrix (75/25 Split)\")\n",
    "plt.savefig(\"../results/rf_confusion_matrix_75_25.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving confusion matrix as CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/rf_confusion_matrix_75_25.csv\")\n",
    "\n",
    "print(\"75/25 Random Forest results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c78ddc",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845fceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    153371\n",
      "     neutral       0.25      0.00      0.00     65563\n",
      "    positive       0.14      0.00      0.00     50212\n",
      "\n",
      "    accuracy                           0.57    269146\n",
      "   macro avg       0.32      0.33      0.24    269146\n",
      "weighted avg       0.41      0.57      0.41    269146\n",
      "\n",
      "\n",
      " Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    153371\n",
      "     neutral       0.29      0.00      0.00     65562\n",
      "    positive       0.18      0.00      0.00     50213\n",
      "\n",
      "    accuracy                           0.57    269146\n",
      "   macro avg       0.35      0.33      0.24    269146\n",
      "weighted avg       0.43      0.57      0.41    269146\n",
      "\n",
      "\n",
      " Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    153371\n",
      "     neutral       0.23      0.00      0.00     65562\n",
      "    positive       0.28      0.00      0.00     50212\n",
      "\n",
      "    accuracy                           0.57    269145\n",
      "   macro avg       0.36      0.33      0.24    269145\n",
      "weighted avg       0.43      0.57      0.41    269145\n",
      "\n",
      "\n",
      " Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    153371\n",
      "     neutral       0.28      0.00      0.00     65562\n",
      "    positive       0.30      0.00      0.00     50212\n",
      "\n",
      "    accuracy                           0.57    269145\n",
      "   macro avg       0.38      0.33      0.24    269145\n",
      "weighted avg       0.45      0.57      0.41    269145\n",
      "\n",
      "\n",
      " Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73    153371\n",
      "     neutral       0.25      0.00      0.00     65562\n",
      "    positive       0.25      0.00      0.00     50212\n",
      "\n",
      "    accuracy                           0.57    269145\n",
      "   macro avg       0.36      0.33      0.24    269145\n",
      "weighted avg       0.43      0.57      0.41    269145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting up 5-fold CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "rf_fold_reports = []\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(f\"\\n Fold {fold}\")\n",
    "    \n",
    "    X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
    "    y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
    "\n",
    "    #Random Forest\n",
    "    rf_cv = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Training\n",
    "    rf_cv.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_cv = rf_cv.predict(X_test_cv)\n",
    "\n",
    "    #Collecting results\n",
    "    report = classification_report(y_test_cv, y_pred_cv, output_dict=True)\n",
    "    rf_fold_reports.append(report)\n",
    "    \n",
    "    print(classification_report(y_test_cv, y_pred_cv))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1298d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Random Forest results saved.\n",
      "\n",
      "{\n",
      "    \"negative\": {\n",
      "        \"precision\": 0.5699,\n",
      "        \"recall\": 0.9995,\n",
      "        \"f1-score\": 0.7259\n",
      "    },\n",
      "    \"neutral\": {\n",
      "        \"precision\": 0.2596,\n",
      "        \"recall\": 0.0004,\n",
      "        \"f1-score\": 0.0008\n",
      "    },\n",
      "    \"positive\": {\n",
      "        \"precision\": 0.2296,\n",
      "        \"recall\": 0.0002,\n",
      "        \"f1-score\": 0.0003\n",
      "    },\n",
      "    \"macro avg F1\": 0.2423,\n",
      "    \"weighted avg F1\": 0.4139\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Labels and metrics\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
    "\n",
    "#Initializing aggregation structure\n",
    "avg_rf_results = {label: {metric: [] for metric in metrics} for label in labels}\n",
    "\n",
    "#Collecting fold-wise results\n",
    "for report in rf_fold_reports:\n",
    "    for label in labels:\n",
    "        for metric in metrics:\n",
    "            avg_rf_results[label][metric].append(report[label][metric])\n",
    "\n",
    "#Computing averages\n",
    "avg_rf_final = {\n",
    "    label: {\n",
    "        metric: round(np.mean(avg_rf_results[label][metric]), 4)\n",
    "        for metric in metrics\n",
    "    }\n",
    "    for label in labels\n",
    "}\n",
    "\n",
    "# Adding macro and weighted F1\n",
    "macro_f1 = np.mean([avg_rf_final[label][\"f1-score\"] for label in labels])\n",
    "weighted_f1s = [report[\"weighted avg\"][\"f1-score\"] for report in rf_fold_reports]\n",
    "\n",
    "avg_rf_final[\"macro avg F1\"] = round(macro_f1, 4)\n",
    "avg_rf_final[\"weighted avg F1\"] = round(np.mean(weighted_f1s), 4)\n",
    "\n",
    "#Saving to JSON\n",
    "with open(\"../results/rf_results_5fold.json\", \"w\") as f:\n",
    "    json.dump(avg_rf_final, f, indent=4)\n",
    "\n",
    "#Summary\n",
    "print(\"5-Fold Random Forest results saved.\\n\")\n",
    "print(json.dumps(avg_rf_final, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0024db0",
   "metadata": {},
   "source": [
    "#### RF Performance\n",
    "The Random Forest classifier performed slightly better than SVM and Logistic Regression because the latter two classifiers are not best to represent non-linear patterns. Nevertheless, the results were quite modest compared to those taking place with high dimensional BERT vectors. It is a confirmation that tree-based methods are able to partially over compensate on physician information being lost, however, they are yet affected by PCA information loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b9484",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "According to all three common models the SVM, the Logistic Regression, and the Random Forest that were tested, there was a consistent decrease in performance when the reduced BERT embeddings were created using the PCA. Though PCA has been used to render the data manageable to these models it seems to have sacrificed the richness of original feature space. This experiment pointed out the trade-offs between a model-sufficiency and a computation-efficiency, and validated that although these old models are the useful comparative baselines, they are not optimally designed to such kind of representation of the data. Such observations aided the reasoning that we should further the use of the BiLSTM model that could directly operate on raw sequential data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
