{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61eebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37124f",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa85efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1345727, 6)\n",
      "                                          clean_text final_sentiment\n",
      "0                  and how do you propose we do that         neutral\n",
      "1  i have sent several private messages and no on...        negative\n",
      "2                      is the worst customer service        negative\n",
      "3  you gonna magically change your connectivity f...         neutral\n",
      "4          since i signed up with you....since day 1         neutral\n"
     ]
    }
   ],
   "source": [
    "#Loading the final cleaned dataset before embeddings and PCA\n",
    "df = pd.read_pickle(\"../data/processed/customer_data_final.pkl\")\n",
    "\n",
    "# Check dataset\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df[['clean_text', 'final_sentiment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146d69c",
   "metadata": {},
   "source": [
    "### TF - IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a4f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1345727, 10000)\n",
      "Example label: neutral\n"
     ]
    }
   ],
   "source": [
    "#Defining TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "#Fiting and transforming the text\n",
    "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "#Getting the labels\n",
    "y = df[\"final_sentiment\"]\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "print(\"Example label:\", y.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfc302",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "364eea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.74      0.81    153371\n",
      "     neutral       0.56      0.77      0.65     65563\n",
      "    positive       0.71      0.76      0.73     50212\n",
      "\n",
      "    accuracy                           0.75    269146\n",
      "   macro avg       0.72      0.76      0.73    269146\n",
      "weighted avg       0.78      0.75      0.76    269146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regretion 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "#Training Logistic Regression\n",
    "logreg_raw = LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=42)\n",
    "logreg_raw.fit(X_train, y_train)\n",
    "\n",
    "#Predicting\n",
    "y_pred_logreg_raw = logreg_raw.predict(X_test)\n",
    "\n",
    "#Evaluating\n",
    "report_logreg_raw = classification_report(y_test, y_pred_logreg_raw, output_dict=True)\n",
    "print(classification_report(y_test, y_pred_logreg_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351edc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression results saved.\n"
     ]
    }
   ],
   "source": [
    "#Saving classification report\n",
    "with open(\"../results/logreg/raw_text/logreg_tfidf_results_80_20.json\", \"w\") as f:\n",
    "    json.dump(report_logreg_raw, f, indent=4)\n",
    "\n",
    "#Confusion matrix\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "cm = confusion_matrix(y_test, y_pred_logreg_raw, labels=labels)\n",
    "\n",
    "#Saving image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression TF-IDF Confusion Matrix (80/20 Split)\")\n",
    "plt.savefig(\"../results/logreg/raw_text/logreg_tfidf_confusion_matrix_80_20.png\")\n",
    "plt.close()\n",
    "\n",
    "#Saving CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/logreg/raw_text/logreg_tfidf_confusion_matrix_80_20.csv\")\n",
    "print(\"TF-IDF Logistic Regression results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cfdc4",
   "metadata": {},
   "source": [
    "####  Logistic Regression with Raw Text Features\n",
    "The fact that the Logistic Regression technique was also effectively leveraged with raw text instead of PCA reduced embedding also proved to be extremely helpful. As demonstrated, the model had better and more reliable accuracy levels across the folds, indicating that simpler delivery indicators such as bag-of-words or TF-IDF can be very competent when pared with interpretable models. This further proves that feature structure and preprocessing amount to more concern than model complexity in a few cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05c7f6",
   "metadata": {},
   "source": [
    "### Support Verctor Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7e340d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.85      0.83    153371\n",
      "     neutral       0.62      0.56      0.59     65563\n",
      "    positive       0.72      0.71      0.71     50212\n",
      "\n",
      "    accuracy                           0.75    269146\n",
      "   macro avg       0.72      0.70      0.71    269146\n",
      "weighted avg       0.75      0.75      0.75    269146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM using SGD 80/20 split\n",
    "svm_tfidf = SGDClassifier(loss='hinge', class_weight='balanced', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "#Training\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "\n",
    "#Predicting\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test)\n",
    "\n",
    "#Evaluating\n",
    "from sklearn.metrics import classification_report\n",
    "report_svm_tfidf = classification_report(y_test, y_pred_svm_tfidf, output_dict=True)\n",
    "print(classification_report(y_test, y_pred_svm_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0095e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF SVM results saved.\n"
     ]
    }
   ],
   "source": [
    "#Saving classification report\n",
    "with open(\"../results/svm/raw_text/svm_tfidf_results_80_20.json\", \"w\") as f:\n",
    "    json.dump(report_svm_tfidf, f, indent=4)\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_svm_tfidf, labels=labels)\n",
    "\n",
    "#Saving image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"SVM TF-IDF Confusion Matrix (80/20 Split)\")\n",
    "plt.savefig(\"../results/svm/raw_text/svm_tfidf_confusion_matrix_80_20.png\")\n",
    "plt.close()\n",
    "\n",
    "#Saving CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/svm/raw_text/svm_tfidf_confusion_matrix_80_20.csv\")\n",
    "print(\"TF-IDF SVM results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed1c23",
   "metadata": {},
   "source": [
    "### SVM with Raw Text Features\n",
    "The raw texts on the vectors features gave significantly better outcomes than reduced BERT embeddings on PCA. This demonstrates the significance of having feature format on the linear models such as SVM, the refinement indicates that although embeddings hold a lot of deep semantic information, they should not be organised in such a manner that does not coincide with the capabilities of the model. The findings suggest that the classical models are capable of a competitive performance in text classification tasks as long as they are vectorised properly as an exemple  using TF-IDF vectoriser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fcfc5d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdbd1b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.86      0.83    153371\n",
      "     neutral       0.63      0.55      0.59     65563\n",
      "    positive       0.75      0.68      0.71     50212\n",
      "\n",
      "    accuracy                           0.75    269146\n",
      "   macro avg       0.73      0.70      0.71    269146\n",
      "weighted avg       0.75      0.75      0.75    269146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest 80/20 split\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "\n",
    "#Training\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "\n",
    "#Predicting\n",
    "y_pred_rf_tfidf = rf_tfidf.predict(X_test)\n",
    "\n",
    "#Evaluating\n",
    "report_rf_tfidf = classification_report(y_test, y_pred_rf_tfidf, output_dict=True)\n",
    "print(classification_report(y_test, y_pred_rf_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a62acbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Random Forest results saved.\n"
     ]
    }
   ],
   "source": [
    "#Saving classification report\n",
    "with open(\"../results/rf/raw_text/rf_tfidf_results_80_20.json\", \"w\") as f:\n",
    "    json.dump(report_rf_tfidf, f, indent=4)\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf_tfidf, labels=labels)\n",
    "\n",
    "#Saving image\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Random Forest TF-IDF Confusion Matrix (80/20 Split)\")\n",
    "plt.savefig(\"../results/rf/raw_text/rf_tfidf_confusion_matrix_80_20.png\")\n",
    "plt.close()\n",
    "\n",
    "#Saving CSV\n",
    "pd.DataFrame(cm, index=labels, columns=labels).to_csv(\"../results/rf/raw_text/rf_tfidf_confusion_matrix_80_20.csv\")\n",
    "print(\"TF-IDF Random Forest results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e7d9e6",
   "metadata": {},
   "source": [
    "#### Random Forest with Raw Text Features\n",
    "Random Forest gave the best result in terms of raw text features amongst the traditional models. The non-linear aspect of the model was useful in grasping the pattern and interaction among the frequencies of words better than it would do with the PCA compressed features. This further supports the notion that good vectorisation is capable of providing simpler models with much needed signal that could translate to good performances when dealing with raw texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d119680",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In conclusion this notebook thus illustrated a sharp increase in the performance of all the traditional Machine Learning models when the model was trained on the vectorised raw text features rather than the PCA reduced embeddings. This arrangement worked much more effectively with SVM, Logistic Regression and, more importantly, Random Forest. The outcome confirms the decision to use direct text vectorisation of the models which are not based on sequence data. This was also a crucial reference point, demonstrating that conventional ML models will not die out and indeed any model can be effective with structured input features in place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
